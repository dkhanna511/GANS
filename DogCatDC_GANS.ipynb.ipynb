{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "DogCatDC_GANS.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FuoAFlX9dUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBy7VwAn-G7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xzvf \"/content/gdrive/My Drive/colab_folder/catdog.tar.gz\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnzgSKrf-R4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os, time, itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "import pickle\n",
        "import matplotlib.gridspec as gridspec\n",
        "from IPython.display import Image, display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOULYZa99dUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIvgoNI09dUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data=\"kagglecatsanddogs_3367a/PetImages\"\n",
        "Categories=[\"Dog\", \"Cat\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQc5KobG9dU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_size= 64\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocAN6f6w9dU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data=[]\n",
        "def create_training_data():\n",
        "    for category in Categories:\n",
        "    \n",
        "        # here we need to join the path of the directories of dogs and cats together\n",
        "        path=os.path.join(data, category)\n",
        "\n",
        "        #here we create catogories for the classes for comparison\n",
        "        class_num=Categories.index(category)\n",
        "\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_array=cv2.imread(os.path.join(path, img),0)  \n",
        "                new_images=cv2.resize(img_array, (img_size, img_size))  # size of image is 100*100\n",
        "                training_data.append([new_images, class_num])\n",
        "                #print(class_num)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "create_training_data()      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9RRP5Dq9dVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(len(training_data))\n",
        "print(np.shape(training_data))\n",
        "print(np.shape(training_data[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4RrcH0m9dVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx=[]\n",
        "yy=[]\n",
        "\n",
        "for features, label in training_data:\n",
        "    xx.append(features)\n",
        "    yy.append(label)\n",
        "plt.imshow(xx[0])\n",
        "\n",
        "#print(yy)\n",
        "\n",
        "xx = np.array(xx)\n",
        "print(xx.shape)\n",
        "print(np.shape(yy))\n",
        "#print(x_main.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ac3T999dVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=(xx.shape[0])\n",
        "i=np.random.permutation(a)\n",
        "#x_train=x_main[i]\n",
        "#print(x_train.shape)\n",
        "\n",
        "x_train = xx[i]\n",
        "print(\"type of x_train is : {}\".format(type(x_train)))\n",
        "print(\" the shape of x_train is : {}\".format(x_train.shape))\n",
        "x_train = x_train.reshape([-1, img_size , img_size, 1])\n",
        "print(\"The new shape  of x_train is : {}\".format(x_train.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfpD2nzW9dVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalising the data by dividing it by the maximum value of pixel\n",
        "\n",
        "#x_train = x_train/255.0\n",
        "totsize = len(training_data)\n",
        "print(totsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6p--JV49dVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdt-0GhX9dVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(x, isTrain = True, reuse = tf.AUTO_REUSE):\n",
        "    with tf.variable_scope('generator', reuse= reuse):\n",
        "        \n",
        "        conv1 = tf.layers.conv2d_transpose(x, 1024, [4,4], strides = (1,1), padding = 'valid')\n",
        "        lrelu1 = lrelu(tf.layers.batch_normalization(conv1, training = isTrain), 0.2)\n",
        "        \n",
        "        conv2 = tf.layers.conv2d_transpose(lrelu1, 512, [4,4], strides = (2,2), padding = 'same')\n",
        "        lrelu2 = lrelu(tf.layers.batch_normalization(conv2, training= isTrain), 0.2)\n",
        "        \n",
        "        conv3 = tf.layers.conv2d_transpose(lrelu2, 256, [4,4], strides = (2,2), padding = 'same')\n",
        "        lrelu3 = lrelu(tf.layers.batch_normalization(conv3, training = isTrain), 0.2)\n",
        "        \n",
        "        conv4 = tf.layers.conv2d_transpose(lrelu3, 128, [4,4], strides = (2,2), padding = 'same')\n",
        "        lrelu4 = lrelu(tf.layers.batch_normalization(conv4, training = isTrain), 0.2)\n",
        "        \n",
        "        conv5 = tf.layers.conv2d_transpose(lrelu4, 1, [4,4], strides = (2,2), padding = 'same')\n",
        "        outputs = tf.nn.tanh(conv5)\n",
        "        \n",
        "        return outputs\n",
        "    \n",
        "def discriminator (x, isTrain = True, reuse = tf.AUTO_REUSE):\n",
        "    with tf.variable_scope('discriminator', reuse = reuse):\n",
        "        \n",
        "        conv1 = tf.layers.conv2d(x, 128, [4,4], strides = (2,2), padding = 'same')\n",
        "        lrelu1 = lrelu(conv1, 0.2)\n",
        "        \n",
        "        conv2 = tf.layers.conv2d(lrelu1, 256, [4,4], strides = (2,2), padding = 'same')\n",
        "        lrelu2 = lrelu(tf.layers.batch_normalization(conv2, training = isTrain), 0.2)\n",
        "        \n",
        "        conv3 = tf.layers.conv2d(lrelu2, 512, [4,4], strides = (2,2), padding = 'same')\n",
        "        lrelu3 = lrelu(tf.layers.batch_normalization(conv3, training = isTrain), 0.2)\n",
        "        \n",
        "        conv4 = tf.layers.conv2d(lrelu3, 1024, [4,4], strides = (2,2), padding = 'same')\n",
        "        lrelu4 = lrelu(tf.layers.batch_normalization(conv4, training = isTrain), 0.2)\n",
        "        \n",
        "        conv5 = tf.layers.conv2d(lrelu4, 1, [4,4], strides = (1,1), padding = 'valid')\n",
        "        \n",
        "        outputs = tf.nn.sigmoid(conv5)\n",
        "        \n",
        "        return outputs, conv5\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8GB2D1m9dV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lrelu(x, th = 0.2):\n",
        "    return tf.maximum(th*x, x)\n",
        "\n",
        "def sampleZ(batch_size):\n",
        "    return np.random.normal(0, 1, (batch_size, 1, 1, 100))\n",
        "\n",
        "def show_images(images):\n",
        "    images = np.reshape(images, [images.shape[0], -1])\n",
        "    sqrt_n = int(np.ceil(np.sqrt(images.shape[0])))\n",
        "    sqrt_img = int(np.ceil(np.sqrt(images.shape[1])))\n",
        "    \n",
        "    fig = plt.figure(figsize = (sqrt_n, sqrt_n))\n",
        "    gs = gridspec.Gridspec(sqrt_n, sqrt_n)\n",
        "    gs.update(wspace = 0.05, hspace = 0.05)\n",
        "    \n",
        "    for i, img in enumerate(images):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect('equal')\n",
        "        pl.imshow(img.reshape([sqrt_img, sqrt_img]))\n",
        "        \n",
        "    return\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOy6w1mQ9dV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32, shape = (None, img_size, img_size, 1))\n",
        "z = tf.placeholder(tf.float32, shape = (None,1, 1, 100))\n",
        "isTrain = tf.placeholder(dtype = tf.bool)\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYtPIhJR9dV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G_z = generator(z, isTrain)\n",
        "print(G_z.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrDh5pqe9dWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_real, D_real_logits = discriminator(x, isTrain)\n",
        "D_fake, D_fake_logits = discriminator(G_z, isTrain, reuse = True)\n",
        "\n",
        "print(D_real_logits.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkcanMvNE0m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb1WqMlxElSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = D_real_logits, \n",
        "                                                                    labels = tf.ones([batch_size, 1, 1, 1])))\n",
        "\n",
        "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = D_fake_logits, \n",
        "                                                                    labels = tf.zeros([batch_size, 1, 1, 1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EKZgeN1EnFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = D_fake_logits, \n",
        "                                                               labels = tf.ones([batch_size, 1, 1,1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN0KbpBZ9dWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')\n",
        "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator')\n",
        "\n",
        "D_solver = tf.train.AdamOptimizer(learning_rate = 0.0002, beta1 = 0.5).minimize(D_loss, var_list = D_vars)\n",
        "G_solver = tf.train.AdamOptimizer(learning_rate = 0.0002, beta1 = 0.5).minimize(G_loss, var_list = G_vars)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT93B28t9dWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJli1ECk9dWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fixed_z_ = np.random.normal(0, 1, (25, 1, 1, 100))\n",
        "\n",
        "def show_result(num_epoch):\n",
        "    test_images = sess.run(G_z, {z : fixed_z_, isTrain : False})\n",
        "    size_figure_grid = 5\n",
        "    \n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize = (5, 5))\n",
        "    \n",
        "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "        \n",
        "    for k in range(size_figure_grid * size_figure_grid):\n",
        "        i = k // size_figure_grid\n",
        "        j = k % size_figure_grid\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(np.reshape(test_images[k], (64, 64)), cmap = 'gray')\n",
        "    \n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha = 'center')\n",
        "    \n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQzZRRBFGf_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchsize = 100\n",
        "tot_batches = len(training_data)//100\n",
        "print(tot_batches)\n",
        "for it in range(15):\n",
        "    show_result((it + 1))\n",
        "    \n",
        "    for iter in range(len(training_data) // 100):\n",
        "        #print(x_train[iter*batchsize:(iter+1)*batchsize].shape)\n",
        "        x_ = x_train[iter*batchsize:(iter+1)*batchsize]\n",
        "        \n",
        "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict = {x: x_, z: sampleZ(100), isTrain: True})\n",
        "        \n",
        "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={z: sampleZ(100), isTrain: True})\n",
        "        \n",
        "        print('Iter : {}'.format(it))\n",
        "        print('D Loss : {:.4}'.format(D_loss_curr))\n",
        "        print('G loss : {:.4}'.format(G_loss_curr))\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzseOkdPHek_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}