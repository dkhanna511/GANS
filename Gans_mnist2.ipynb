{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_dataini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the discriminator fucntion\n",
    "\n",
    "def dicriminator(x_image, reuse=None):\n",
    "    with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        #defining weights\n",
    "        \n",
    "        #Weights for first conv layer\n",
    "        d_w1 = tf.get_variable('d_w1', [5, 5, 1, 32], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        #weights for second conv layer\n",
    "        d_w2 = tf.get_variable('d_w2', [5, 5, 32, 64], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        #Weights for fully connected layer\n",
    "        d_wfc = tf.get_variable('d_wfc', [7*7*64, 5, 1024 ], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        #Weights for output layer\n",
    "        d_wout = tf.get_variable('d_wout', [1024 , 10 ], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        \n",
    "        #defing biases\n",
    "        \n",
    "        #bias for first conv layer\n",
    "        d_b1 = tf.get_variable('d_b1', [32], initializer=tf.constant_initializer(0))\n",
    "        #bias for second conv layer\n",
    "        d_b2 = tf.get_variable('d_b2', [64], initializer=tf.constant_initializer(0))\n",
    "        #bias for fully connected layer\n",
    "        d_bfc = tf.get_variable('d_bfc', [1024], initializer=tf.constant_initializer(0))\n",
    "        #bias for output layer\n",
    "        d_bout = tf.get_variable('d_bout', [10], initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        #formation of the neural network layers\n",
    "        \n",
    "        #first convolutional layer\n",
    "        d1 = tf.nn.conv2d(input=x_image, filter=d_w1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        d1 = d1 + d_b1\n",
    "        d1 = tf.nn.relu(d1)\n",
    "        d1 = tf.nn.avg_pool(d1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        #second convolutonal layer\n",
    "        d2 = tf.nn.conv2d(input=d1, filter=d_w2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        d2 = d2 + d_b2\n",
    "        d2 = tf.nn.relu(d2)\n",
    "        d2 = tf.nn.avg_pool(d2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        #fully connected layer\n",
    "        d_fc = tf.reshape(d2, [-1, 7 * 7 * 64])\n",
    "        d_fc = tf.matmul(dfc, d_wfc)\n",
    "        d_fc = d_wfc + d_bfc\n",
    "        d_fc = tf.nn.relu(d3)\n",
    "        \n",
    "        #output layer\n",
    "        d_out = tf.matmul(d_wfc, d_wout) + d_bout\n",
    "        \n",
    "        return d_out\n",
    "\n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batch_size, z_dim):\n",
    "    \n",
    "    z = tf.truncated_normal([batch_size, z_dim], mean=0, stddev=1, name='z')\n",
    "    \n",
    "    #defining weights\n",
    "    \n",
    "    #Weights for first deconv layer\n",
    "    g_w1 = tf.get_variable('g_w1', [z_dim, 3136], dtype=tf.float32,\n",
    "                           initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    \n",
    "    #Weights for second deconv layer\n",
    "    g_w2 = tf.get_variable('g_w2', [3, 3, 1, z_dim/2], dtype=tf.float32,\n",
    "                           initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    \n",
    "    #Weights for third deconv layer\n",
    "    g_w3 = tf.get_variable('g_w3', [3, 3, z_dim/2, z_dim/4], dtype=tf.float32, \n",
    "                           initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    \n",
    "    # Final convolution with one output channel\n",
    "    g_w4 = tf.get_variable('g_w4', [1, 1, z_dim/4, 1], dtype=tf.float32,\n",
    "                           initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    \n",
    "     \n",
    "    #Defining biases\n",
    "    \n",
    "    #bias for first deconv layer\n",
    "    g_b1 = tf.get_variable('g_b1', [3136], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    #bias for second deconv layer\n",
    "    g_b2 = tf.get_variable('g_b2', [z_dim/2], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    #bias for third deconv layer\n",
    "    g_b3 = tf.get_variable('g_b3', [z_dim/4], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    #bias for final deconv layer\n",
    "    g_b4 = tf.get_variable('g_b4', [1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    \n",
    "    #formation of neural network\n",
    "    \n",
    "    g1 = tf.matmul(z, g_w1) + g_b1\n",
    "    g1 = tf.reshape(g1, [-1, 56, 56, 1])\n",
    "    g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='bn1')\n",
    "    g1 = tf.nn.relu(g1)\n",
    "    \n",
    "    g2 = tf.nn.conv2d(g1, g_w2, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g2 = g2 + g_b2\n",
    "    g2 = tf.contrib.layers.batch_norm(g2, epsilon=1e-5, scope='bn2')\n",
    "    g2 = tf.nn.relu(g2)\n",
    "    g2 = tf.image.resize_images(g2, [56, 56])\n",
    "    \n",
    "    g3 = tf.nn.conv2d(g2, g_w3, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g3 = g3 + g_b3\n",
    "    g3 = tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope='bn3')\n",
    "    g3 = tf.nn.relu(g3)\n",
    "    g3 = tf.image.resize_images(g3, [56, 56])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
